<h3><b>Basics of text embedding</h3>
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
<h2>One Hot encoding</h2>
from sklearn.preprocessing import LabelBinarizer

corpus = ['apple', 'banana', 'orange', 'apple']
lb = LabelBinarizer()
one_hot = lb.fit_transform(corpus)

print("Vocabulary:", lb.classes_)
print("One-Hot Encoding:\n", one_hot)
<h2>Bag of Words(Count Vectorizer)</h2>
from sklearn.feature_extraction.text import CountVectorizer

corpus = [
    'apple and banana',
    'banana and orange',
    'apple apple banana'
]

vectorizer = CountVectorizer()
bow = vectorizer.fit_transform(corpus)

print("Vocabulary:", vectorizer.get_feature_names_out())
print("BoW Matrix:\n", bow.toarray())
<h2>Tf-Idf Vectorizer</h2>
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vectorizer = TfidfVectorizer()
tfidf = tfidf_vectorizer.fit_transform(corpus)

print("Vocabulary:", tfidf_vectorizer.get_feature_names_out())
print("TF-IDF Matrix:\n", tfidf.toarray())
bow_df = pd.DataFrame(bow.toarray(), columns=vectorizer.get_feature_names_out())
plt.figure(figsize=(8, 4))
sns.heatmap(bow_df, annot=True, cmap="Blues", cbar=False)
plt.title("Bag of Words Representation")
plt.xlabel("Words")
plt.ylabel("Documents")
plt.show()
tfidf_df = pd.DataFrame(tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())
plt.figure(figsize=(8, 4))
sns.heatmap(tfidf_df, annot=True, cmap="Blues", cbar=False)
plt.title("Bag of Words Representation")
plt.xlabel("Words")
plt.ylabel("Documents")
plt.show()
<h2>Neural methods</h2>
!pip install gensim
!pip install --upgrade numpy
!pip install --upgrade gensim
from gensim.models import Word2Vec

# Sample corpus (list of tokenized sentences)
corpus = [
    ['apple', 'is', 'a', 'fruit'],
    ['banana', 'is', 'yellow'],
    ['orange', 'and', 'apple', 'are', 'fruits'],
    ['i', 'love', 'banana', 'and', 'orange']
]

# Train Word2Vec Model - CBOW (default)
model_cbow = Word2Vec(sentences=corpus, vector_size=50, window=2, min_count=1, sg=0)

# Train Word2Vec Model - Skip-Gram
model_skipgram = Word2Vec(sentences=corpus, vector_size=50, window=2, min_count=1, sg=1)

# Example: Get vector for word 'apple'
print("CBOW vector for 'apple':\n", model_cbow.wv['apple'])
print("\nSkip-Gram vector for 'apple':\n", model_skipgram.wv['apple'])

# Most similar words to 'apple'
print("\nCBOW similar to 'apple':", model_cbow.wv.most_similar('apple'))
print("Skip-Gram similar to 'apple':", model_skipgram.wv.most_similar('apple'))

<h3><b>RNN</h3>
<h2>text classification</h2>
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, random_split,TensorDataset
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
import re
data = pd.read_csv("/content/drive/MyDrive/DLL/Restaurant_Reviews.tsv",delimiter='\t')
data.head()
def preprocess(text):
  text = text.lower()
  text = re.sub(r'[^a-z0-9\s]','',text)
  return text
texts = data['Review'].apply(preprocess).tolist()
labels = data['Liked'].tolist()
train_texts, temp_texts, train_labels, temp_labels = train_test_split(texts, labels, test_size=0.3)
val_texts, test_texts, val_labels, test_labels = train_test_split(temp_texts, temp_labels, test_size=0.5)
word_to_idx = {"<PAD>": 0, "<UNK>": 1}
for sent in train_texts:
    for word in sent.split():
        if word not in word_to_idx:
            word_to_idx[word] = len(word_to_idx)

max_len = max(len(sent.split()) for sent in train_texts)

def encode_sentence(sent):
    encoded = [word_to_idx.get(word, word_to_idx["<UNK>"]) for word in sent.split()[:max_len]]
    encoded += [0] * (max_len - len(encoded))
    return encoded
X_train = torch.tensor([encode_sentence(sent) for sent in train_texts], dtype=torch.long)
X_val = torch.tensor([encode_sentence(sent) for sent in val_texts], dtype=torch.long)
X_test = torch.tensor([encode_sentence(sent) for sent in test_texts], dtype=torch.long)
"""
le = LabelEncoder()
le.fit(train_labels+test_labels+val_labels)

y_train = torch.tensor(le.transform(train_labels), dtype=torch.long)
y_val = torch.tensor(le.transform(val_labels), dtype=torch.long)
y_test = torch.tensor(le.transform(test_labels), dtype=torch.long)
"""

y_train = torch.tensor(train_labels, dtype=torch.float32)
y_val = torch.tensor(val_labels, dtype=torch.float32)
y_test = torch.tensor(test_labels, dtype=torch.float32)

train_dataset = TensorDataset(X_train, y_train)
val_dataset = TensorDataset(X_val, y_val)
test_dataset = TensorDataset(X_test, y_test)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)
test_loader = DataLoader(test_dataset, batch_size=32)
class RNNClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim):
        super(RNNClassifier, self).__init__()
        self.embedding = nn.Embedding(num_embeddings=vocab_size,embedding_dim=embed_dim, padding_idx=0)
        self.rnn = nn.RNN(input_size=embed_dim,hidden_size=hidden_dim,num_layers=2, batch_first=True)
        #self.lstm = nn.LSTM(input_size=embed_dim,hidden_size=hidden_dim,num_layers=2, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 1)
        self.act = nn.Sigmoid()

    def forward(self, x):
        x = self.embedding(x)
        _, hidden = self.rnn(x)
        #_, (hidden, _) = self.lstm(x)
        layer1 = self.fc(hidden[-1])
        return self.act(layer1)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = RNNClassifier(len(word_to_idx), embed_dim=64, hidden_dim=64).to(device)
criterion = nn.BCELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
def train(model, loader):
    model.train()
    total_loss = 0
    for x, y in loader:
        x, y = x.to(device), y.to(device)
        optimizer.zero_grad()
        outputs = model(x).squeeze()
        loss = criterion(outputs,y)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(loader)
def evaluate(model, loader):
    predicted = []
    actual = []
    model.eval()
    correct = total = 0
    with torch.no_grad():
        for x, y in loader:
            x, y = x.to(device), y.to(device)
            outputs = model(x).squeeze()
            preds = (outputs>=0.5).float()
            correct += (preds == y).sum().item()
            total += y.size(0)
            predicted.extend(preds.numpy())
            actual.extend(y.float().numpy())
    return predicted,actual,correct / total *100
for epoch in range(100):
    train_loss = train(model, train_loader)
    a,b,val_acc = evaluate(model, val_loader)
    print(f"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Accuracy = {val_acc:.4f}")

y_pred,y_act,test_acc = evaluate(model, test_loader)
print(f"\nTest Accuracy: {test_acc:.4f}")
Text or next word prediction
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader,TensorDataset
import numpy as np

data = pd.read_csv("/content/drive/MyDrive/DLL/Restaurant_Reviews.tsv",delimiter='\t')
def preprocess(text):
  text = text.lower()
  text = re.sub(r'[^a-z0-9\s]','',text)
  return text

corpus = data['Review'].apply(preprocess).tolist()
train_sentences, temp_sentences = train_test_split(corpus, test_size=0.3, random_state=42)
val_sentences, test_sentences = train_test_split(temp_sentences, test_size=0.5, random_state=42)
word_to_idx = {"<PAD>": 0, "<UNK>": 1}
for sentence in train_sentences:
    for word in sentence.split():
        if word not in word_to_idx:
            word_to_idx[word] = len(word_to_idx)

idx_to_word = {idx: word for word, idx in word_to_idx.items()}
vocab_size = len(word_to_idx)
print(f"Vocab size: {vocab_size}")
sequence_length = 2  # how many words used to predict the next

def make_sequences(sentences, seq_len):
    sequences = []
    for sentence in sentences:
        words = sentence.split()
        if len(words) < seq_len + 1:
            continue
        for i in range(len(words) - seq_len):
            input_seq = words[i:i+seq_len]
            target = words[i+seq_len]
            sequences.append((input_seq, target))
    return sequences

train_seq = make_sequences(train_sentences, sequence_length)
val_seq = make_sequences(val_sentences, sequence_length)
test_seq = make_sequences(test_sentences, sequence_length)
def encode(seq):
    return [word_to_idx.get(word, word_to_idx["<UNK>"]) for word in seq]

def prepare_dataset(sequences):
    X = [encode(seq) for seq, _ in sequences]
    y = [word_to_idx.get(target, word_to_idx["<UNK>"]) for _, target in sequences]
    X = torch.tensor(X, dtype=torch.long)
    y = torch.tensor(y, dtype=torch.long)
    return TensorDataset(X, y)

train_dataset = prepare_dataset(train_seq)
val_dataset = prepare_dataset(val_seq)
test_dataset = prepare_dataset(test_seq)

train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=32)
test_loader = DataLoader(test_dataset, batch_size=32)
class NextWordRNN(nn.Module):
    def __init__(self, vocab_size, embed_dim=64, hidden_dim=128):
        super(NextWordRNN, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        #self.rnn = nn.RNN(embed_dim, hidden_dim, batch_first=True,num_layers=2)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True,num_layers=2)
        self.fc = nn.Linear(hidden_dim, vocab_size)  # Predict next word from hidden state

    def forward(self, x):
        x = self.embedding(x)              # (batch_size, seq_len, embed_dim)
        #_, h_n = self.rnn(x)               # h_n: (num_layers * num_directions, batch, hidden_size)
        _,(h_n,_) = self.lstm(x)
        logits = self.fc(h_n[-1])          # Use final hidden state
        return logits                      # (batch, vocab_size)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = NextWordRNN(vocab_size).to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=0.005)
criterion = nn.CrossEntropyLoss()
def train(model, loader):
    model.train()
    total_loss = 0
    for X_batch, y_batch in loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        optimizer.zero_grad()
        outputs = model(X_batch)                     # (batch, vocab_size)
        loss = criterion(outputs, y_batch)           # CrossEntropy expects raw logits
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(loader)
def evaluate(model, loader):
    model.eval()
    correct = total = 0
    predicted = []
    actual = []
    with torch.no_grad():
        for X_batch, y_batch in loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            outputs = model(X_batch)
            preds = outputs.argmax(dim=1)           # Choose the word with highest logit
            correct += (preds == y_batch).sum().item()
            total += y_batch.size(0)
            predicted.extend(preds.float().numpy())
            actual.extend(y_batch.float().numpy())
    return predicted,actual,correct / total * 100
for epoch in range(200):
    train_loss = train(model, train_loader)
    a,b,val_acc = evaluate(model, val_loader)
    print(f"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Accuracy = {val_acc:.2f}%")

y_pred,y_act,test_acc = evaluate(model, test_loader)
print(f"\nTest Accuracy: {test_acc:.2f}%")
def predict_next_word(model, input_sentence, word_to_idx, idx_to_word, sequence_length, device):
    model.eval()
    words = input_sentence.lower().split()
    if len(words) < sequence_length:
        words = ["<PAD>"] * (sequence_length - len(words)) + words
    else:
        words = words[-sequence_length:]

    encoded = [word_to_idx.get(word, word_to_idx["<UNK>"]) for word in words]
    input_tensor = torch.tensor([encoded], dtype=torch.long).to(device)

    with torch.no_grad():
        logits = model(input_tensor)
        pred_idx = torch.argmax(logits, dim=1).item()

    return idx_to_word.get(pred_idx, "<UNK>")

input_sentence = "i love this"
next_word = predict_next_word(model, input_sentence, word_to_idx, idx_to_word, sequence_length, device)
print(f"Next word prediction: {next_word}")
Time Series Data
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
import matplotlib.pyplot as plt

# Create sine wave data
x = np.linspace(0, 100, 1000)
y = np.sin(x)

plt.plot(x, y)
plt.title("Sine Wave")
plt.show()
sequence_length = 2

def create_sequences(data, seq_len):
    X, y = [], []
    for i in range(len(data) - seq_len):
        X.append(data[i:i+seq_len])
        y.append(data[i+seq_len])
    return np.array(X), np.array(y)

X, y = create_sequences(y, sequence_length)
print(X[0:5],y[0:5])
train_size = int(0.7 * len(X))
val_size = int(0.15 * len(X))
test_size = len(X) - train_size - val_size

X_train, X_val, X_test = X[:train_size], X[train_size:train_size+val_size], X[train_size+val_size:]
y_train, y_val, y_test = y[:train_size], y[train_size:train_size+val_size], y[train_size+val_size:]
print(X_train.shape)
X_train = torch.tensor(X_train, dtype=torch.float32).unsqueeze(-1)
y_train = torch.tensor(y_train, dtype=torch.float32).unsqueeze(-1)
X_val = torch.tensor(X_val, dtype=torch.float32).unsqueeze(-1)
y_val = torch.tensor(y_val, dtype=torch.float32).unsqueeze(-1)
X_test = torch.tensor(X_test, dtype=torch.float32).unsqueeze(-1)
y_test = torch.tensor(y_test, dtype=torch.float32).unsqueeze(-1)
print(X_train.shape)
train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=32)
val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=32)
test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=32)

class RNNTimeSeries(nn.Module):
    def __init__(self, input_dim=1, hidden_dim=64, num_layers=2):
        super(RNNTimeSeries, self).__init__()
        #self.rnn = nn.RNN(input_dim, hidden_dim, num_layers=num_layers, batch_first=True)
        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, 1)

    def forward(self, x):
        out, _ = self.lstm(x)
        return self.fc(out[:, -1, :])  # only final time step output
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = RNNTimeSeries().to(device)

criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
def train(model, loader):
    model.train()
    total_loss = 0
    for X_batch, y_batch in loader:
        X_batch, y_batch = X_batch.to(device), y_batch.to(device)
        optimizer.zero_grad()
        output = model(X_batch)
        loss = criterion(output, y_batch)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(loader)
def evaluate(model, loader):
    model.eval()
    total_loss = 0
    with torch.no_grad():
        for X_batch, y_batch in loader:
            X_batch, y_batch = X_batch.to(device), y_batch.to(device)
            output = model(X_batch)
            loss = criterion(output, y_batch)
            total_loss += loss.item()
    return total_loss / len(loader)
for epoch in range(50):
    train_loss = train(model, train_loader)
    val_loss = evaluate(model, val_loader)
    print(f"Epoch {epoch+1}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}")
model.eval()
with torch.no_grad():
    y_pred = model(X_test.to(device)).cpu().numpy()
    y_true = y_test.numpy()

plt.plot(y_true, label='Actual')
plt.plot(y_pred, label='Predicted')
plt.legend()
plt.title("Sine Wave Prediction")
plt.show()
